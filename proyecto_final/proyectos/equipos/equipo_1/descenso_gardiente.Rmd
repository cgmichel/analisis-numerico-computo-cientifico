---
title: "M_descenso_gradiente"
author: "Paulina Gomez Mont W"
date: "5/1/2019"
output: html_document
---

#cargamos las librerias
```{r,warning=FALSE,message=FALSE,echo=FALSE,error=FALSE}
library(nnet)
library(tidyverse)
```

Leemos los datos
```{r}
datos <- read_csv("./train-2.csv")

```
Definimos las funciones que usaremos para descenso en gradiente.
```{r}
#Funcion para calculo de la funcion multinomial
pred_ml <- function(x, beta){
  p <- ncol(x)
  K <- length(beta)/(p+1) + 1
  beta_mat <- matrix(beta, K - 1, p + 1 , byrow = TRUE)
  u_beta <- exp(as.matrix(cbind(1, x)) %*% t(beta_mat))
  Z <- 1 + apply(u_beta, 1, sum)
  p_beta <- cbind(u_beta, 1)/Z
  as.matrix(p_beta)
}
#Funcion para calcular la devianza
devianza_calc <- function(x, y){
  dev_fun <- function(beta){
    p_beta <- pred_ml(x, beta)
    p <- sapply(1:nrow(x), function(i) p_beta[i, y[i]+1])
   -2*sum(log(p))
  }
  dev_fun
}
#Funcion para calcular el gradiente
grad_calc <- function(x_ent, y_ent){
  p <- ncol(x_ent)
  K <- length(unique(y_ent)) 
  y_fact <- factor(y_ent) 
  # matriz de indicadoras de clase
  y_dummy <-  model.matrix(~-1 + y_fact)

  salida_grad <- function(beta){
    p_beta <-  pred_ml(x_ent, beta)
    e_mat <-  (y_dummy  - p_beta)[, -K]
    grad_out <- -2*(t(cbind(1,x_ent)) %*% e_mat)
    as.numeric(grad_out)
  }
  salida_grad
}
# Hacemos descenso en gradiente n veces con un paso de tamaño eta, h_deriv es el gradiente, dev_fun es la funcion de devianza
descenso <- function(n, z_0, eta, h_deriv, dev_fun){
  z <- matrix(0,n, length(z_0))
  z[1, ] <- z_0
  for(i in 1:(n-1)){
    z[i+1, ] <- z[i, ] - eta * h_deriv(z[i, ])
    if(i %% 100 == 0){
      print(paste0(i, ' Devianza: ', dev_fun(z[i+1, ])))
    }
  }
  z
}
```

```{r}
#dividimos en prueba y entrenamiento
set.seed(530)
ent <- sample_frac(datos,0.5) %>% data.frame()
prueba <- anti_join(datos,ent)

```

Manipulamos los datos de entrenamiento y calculamos las funciones
```{r}
#definimos las variables independientes
x_ent <- ent %>% select(-c(experiment,event)) %>% as.matrix
# definimos la variable respuesta
y_ent <- ent %>% select(c(event)) 
#escalamos las variables x
x_ent_s <- x_ent %>% scale()
y_ent$event[y_ent$event == "A"] <- 0
y_ent$event[y_ent$event == "B"] <- 1
y_ent$event[y_ent$event == "C"] <- 2
y_ent$event[y_ent$event == "D"] <- 3
y_ent <- unlist(y_ent) %>% as.numeric()

#definimla beta inicial
beta <- c(4,rep(1, ncol(x_ent)*3-1))
#la funcion de devianza con los datos de entrenamiento
dev_ent <- devianza_calc(x_ent_s, y_ent)
#la funcion de gradiente con los datos de entrenamiento
grad <- grad_calc(x_ent_s, y_ent)


```
Primera implementación 1900 iteraciones (pasos) con un tamaño de paso de $5\times 10^7$
```{r}
# Primera implementación 1900 iteraciones (pasos) con un tamaño de paso de 5x10^7
z <- descenso(1900, c(-2,rep(-0.05, (ncol(x_ent_s)+1)*3-1)), eta=0.0000005, 
                        h_deriv = grad, dev_fun = dev_ent)
```
Imprimimos la Beta obtenida con la Primera Implementación, la devianza correspondiente y la norma del gradiente
```{r}

print("Las Betas son:")
z[1900,]
print("El valor de la norma del gradiente es:")
grad(z[1900,]) %>% norm(type="2")
print("El valor de la devianza es:")
dev_ent(z[1900,])

```

Segunda implementación 3000 iteraciones (pasos) con un tamaño de paso de $1\times 10^7$
```{r}
#dev_ent(z[1900,])
beta <- z[1900,]
z_1 <- descenso(3000, beta, eta=0.0000001, 
                        h_deriv = grad, dev_fun = dev_ent)
```
Imprimimos la Beta obtenida con la Segunda Implementación, la devianza correspondiente y la norma del gradiente
```{r}
print("Las Betas son:")
z_1[3000,]
print("El valor de la norma del gradiente es:")
grad(z_1[3000,]) %>% norm(type="2")
print("El valor de la devianza es:")
dev_ent(z_1[3000,])
```


Tercera implementación 1000 iteraciones (pasos) con un tamaño de paso de $5\times 10^8$
```{r}
beta <- z_1[3000,]
z_2 <- descenso(1000, beta, eta=0.00000005, 
                        h_deriv = grad, dev_fun = dev_ent)
```

Imprimimos la Beta obtenida con la Tercera Implementación, la devianza correspondiente y la norma del gradiente
```{r}
print("Las Betas son:")
z_2[1000,]
print("El valor de la norma del gradiente es:")
grad(z_2[1000,]) %>% norm(type="2")
print("El valor de la devianza es:")
dev_ent(z_2[1000,])
```


Contrastamos los resultados obtenidos con el algoritmo de multinom de la libreria nnet

```{r}

mod_mult <- multinom(y_ent ~ x_ent_s, data = ent, MaxNWt=100000, maxit = 500)
mod_mult
```

